package main

import (
	"encoding/json"
	"flag"
	"fmt"
	"log"
	"os"
	"path/filepath"
	"time"

	"github.com/AlexHubble/research-solana/confluent/pkg/config"
	"github.com/confluentinc/confluent-kafka-go/v2/kafka"
)

// Message æ¶ˆæ¯ç»“æ„ä½“
type Message struct {
	ID        int                    `json:"id"`
	Timestamp string                 `json:"timestamp"`
	Message   string                 `json:"message"`
	Producer  string                 `json:"producer"`
	Data      map[string]interface{} `json:"data"`
}

func main() {
	// å‘½ä»¤è¡Œå‚æ•°
	var (
		configPath = flag.String("config", "../config/kafka.yaml", "é…ç½®æ–‡ä»¶è·¯å¾„")
		count      = flag.Int("count", 10, "å‘é€æ¶ˆæ¯æ•°é‡")
		interval   = flag.Duration("interval", time.Second, "å‘é€é—´éš”")
	)
	flag.Parse()

	// è·å–é…ç½®æ–‡ä»¶ç»å¯¹è·¯å¾„
	if !filepath.IsAbs(*configPath) {
		wd, err := os.Getwd()
		if err != nil {
			log.Fatalf("âŒ è·å–å·¥ä½œç›®å½•å¤±è´¥: %v", err)
		}
		*configPath = filepath.Join(wd, *configPath)
	}

	// åŠ è½½é…ç½®
	fmt.Printf("ğŸ“– åŠ è½½é…ç½®æ–‡ä»¶: %s\n", *configPath)
	cfg, err := config.LoadConfig(*configPath)
	if err != nil {
		log.Fatalf("âŒ åŠ è½½é…ç½®å¤±è´¥: %v", err)
	}

	// åˆ›å»ºProducer
	fmt.Println("ğŸš€ åˆ›å»ºKafka Producer...")
	producer, err := kafka.NewProducer(cfg.GetProducerConfigMap())
	if err != nil {
		log.Fatalf("âŒ åˆ›å»ºProducerå¤±è´¥: %v", err)
	}
	defer producer.Close()

	// å¯åŠ¨äº‹ä»¶å¤„ç†goroutine
	go handleEvents(producer)

	topic := cfg.Producer.Topic
	fmt.Printf("ğŸ“¤ å¼€å§‹å‘é€æ¶ˆæ¯åˆ°topic: %s\n", topic)

	// å‘é€æ¶ˆæ¯
	for i := 0; i < *count; i++ {
		message := Message{
			ID:        i + 1,
			Timestamp: time.Now().Format(time.RFC3339),
			Message:   fmt.Sprintf("è¿™æ˜¯ç¬¬ %d æ¡æµ‹è¯•æ¶ˆæ¯", i+1),
			Producer:  "confluent-demo-producer-go",
			Data: map[string]interface{}{
				"sequence":   i + 1,
				"batch_size": *count,
			},
		}

		// åºåˆ—åŒ–æ¶ˆæ¯
		messageBytes, err := json.Marshal(message)
		if err != nil {
			log.Printf("âŒ åºåˆ—åŒ–æ¶ˆæ¯å¤±è´¥: %v", err)
			continue
		}

		// å‘é€æ¶ˆæ¯
		fmt.Printf("ğŸ“¨ å‘é€æ¶ˆæ¯ %d/%d\n", i+1, *count)
		err = producer.Produce(&kafka.Message{
			TopicPartition: kafka.TopicPartition{
				Topic:     &topic,
				Partition: kafka.PartitionAny,
			},
			Value: messageBytes,
		}, nil)

		if err != nil {
			log.Printf("âŒ å‘é€æ¶ˆæ¯å¤±è´¥: %v", err)
			continue
		}

		// ç­‰å¾…é—´éš”ï¼ˆæœ€åä¸€æ¡æ¶ˆæ¯ä¸éœ€è¦ç­‰å¾…ï¼‰
		if i < *count-1 {
			time.Sleep(*interval)
		}
	}

	// ç­‰å¾…æ‰€æœ‰æ¶ˆæ¯å‘é€å®Œæˆ
	fmt.Println("â³ ç­‰å¾…æ‰€æœ‰æ¶ˆæ¯å‘é€å®Œæˆ...")
	producer.Flush(15 * 1000) // 15ç§’è¶…æ—¶

	fmt.Println("âœ… æ‰€æœ‰æ¶ˆæ¯å‘é€å®Œæˆ!")
}

// handleEvents å¤„ç†Produceräº‹ä»¶
func handleEvents(producer *kafka.Producer) {
	for e := range producer.Events() {
		switch ev := e.(type) {
		case *kafka.Message:
			if ev.TopicPartition.Error != nil {
				fmt.Printf("âŒ æ¶ˆæ¯å‘é€å¤±è´¥: %v\n", ev.TopicPartition.Error)
			} else {
				fmt.Printf("âœ… æ¶ˆæ¯å‘é€æˆåŠŸ: topic=%s, partition=%d, offset=%v\n",
					*ev.TopicPartition.Topic,
					ev.TopicPartition.Partition,
					ev.TopicPartition.Offset)
			}
		case kafka.Error:
			fmt.Printf("âŒ Kafkaé”™è¯¯: %v\n", ev)
		default:
			// å¿½ç•¥å…¶ä»–äº‹ä»¶ç±»å‹
		}
	}
}
